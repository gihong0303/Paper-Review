# LLM 논문 리뷰 컬렉션

대규모 언어 모델(LLM) 관련 핵심 논문 전체적인 리뷰

## 목차

### Part 1. System & Inference Optimization (시스템 및 추론 최적화)
속도, 메모리 효율성, 서빙 엔진, 양자화 기술
- [1. Kernel & Attention Optimization](./Part1_System_and_Inference_Optimization/1_Kernel_and_Attention_Optimization/)
- [2. Quantization (양자화)](./Part1_System_and_Inference_Optimization/2_Quantization/)
- [3. Speculative Decoding & Speed](./Part1_System_and_Inference_Optimization/3_Speculative_Decoding/)

### Part 2. Advanced Fine-tuning & Training Methodologies
PEFT, SFT, Alignment(RLHF/DPO), 분산 학습
- [1. PEFT (Parameter-Efficient Fine-Tuning)](./Part2_Advanced_Finetuning/1_PEFT/)
- [2. Alignment & Preference Learning](./Part2_Advanced_Finetuning/2_Alignment/)
- [3. Instruction Tuning & Data Selection](./Part2_Advanced_Finetuning/3_Instruction_Tuning/)
- [4. Distributed Training](./Part2_Advanced_Finetuning/4_Distributed_Training/)

### Part 3. Domain Adaptation & Data Engineering
CPT, 데이터 엔지니어링, 토크나이저 확장, 합성 데이터
- [1. Domain Adaptation & CPT](./Part3_Domain_Adaptation/1_Domain_Adaptation_CPT/)
- [2. Data Engineering & Synthetic Data](./Part3_Domain_Adaptation/2_Data_Engineering/)
- [3. Multilingual & Tokenizer](./Part3_Domain_Adaptation/3_Multilingual_Tokenizer/)

### Part 4. LLM Architecture & Theory
Transformer, MoE, Positional Embedding, Post-Transformer
- [1. Foundation & Scaling Laws](./Part4_Architecture_Theory/1_Foundation_Scaling/)
- [2. Positional Embeddings & Attention Variants](./Part4_Architecture_Theory/2_Positional_Attention/)
- [3. Mixture of Experts (MoE)](./Part4_Architecture_Theory/3_MoE/)
- [4. Post-Transformer Architectures](./Part4_Architecture_Theory/4_Post_Transformer/)
- [5. Reasoning & Chain of Thought](./Part4_Architecture_Theory/5_Reasoning_CoT/)
