# AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

**논문 발표**: 2023년 (MLSys 2024)
**저자**: Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han
**소속**: MIT, NVIDIA
**논문 링크**: [arXiv:2306.00978](https://arxiv.org/abs/2306.00978)
**공식 구현**: [GitHub](https://github.com/mit-han-lab/llm-awq)

---

## 한 줄 요약
> 활성화(Activation) 크기를 기준으로 중요한 가중치(Salient Weights)를 식별하고, 이를 보호하는 per-channel scaling으로 GPTQ보다 빠르게 양자화하면서 동등 이상의 정확도 달성

---

## 1. 핵심 통찰: 모든 가중치가 동일하게 중요하지 않다

### 1.1 관찰

가중치의 **0.1-1%** 만 건드려도 perplexity가 급격히 증가:

```
실험: LLaMA-7B에서 가중치의 일부를 0으로

가중치 %  | Perplexity
----------|------------
0%        | 5.68 (원본)
0.01%     | 6.22
0.1%      | 43.22 (10배 증가!)
1%        | 5e5 (발산)
```

**결론**: 소수의 "salient weights"가 매우 중요

### 1.2 중요한 가중치란?

> "큰 활성화와 곱해지는 가중치가 중요하다!"

수학적 직관:
$$Y = WX$$

$X_i$가 크면 → $W_{:,i}$의 양자화 오류가 $Y$에 크게 영향

### 1.3 시각화

```
활성화 X:     [0.1, 0.2, 10.0, 0.3, 0.1]
               ↓    ↓    ↓     ↓    ↓
가중치 중요도: [낮음][낮음][높음!][낮음][낮음]

→ 세 번째 채널의 가중치가 가장 중요
```

---

## 2. 기존 접근법의 문제

### 2.1 Mixed-precision (LLM.int8() 방식)

중요한 가중치를 FP16으로 유지:

```python
# 문제: 하드웨어 비효율
if is_salient(weight):
    use_fp16()  # 별도 처리
else:
    use_int4()
```

**문제점**:
- 혼합 정밀도 → 하드웨어 비효율
- INT4 커널 최적화 어려움

### 2.2 GPTQ의 한계

- 양자화 시간이 오래 걸림 (Hessian 계산)
- 최적화 기반이라 직관적이지 않음

---

## 3. AWQ의 해결책: Per-channel Scaling

### 3.1 핵심 아이디어

중요한 채널의 가중치를 **스케일링**하여 양자화 오류 감소:

```python
# 스케일링 적용
W_scaled = W * s          # 가중치에 곱하기
X_scaled = X / s          # 활성화에 나누기

# 결과는 동일
Y = W @ X = (W * s) @ (X / s) = W_scaled @ X_scaled

# 하지만 양자화 오류는 감소!
```

### 3.2 왜 효과적인가?

```
원본:
W = [0.5, 0.3, 0.4]  # 양자화 후 [0, 0, 0]
X = [0.1, 0.1, 10.0]
→ 출력 오류 = 4.0 (큰 활성화 × 양자화 오류)

스케일링 (s = [1, 1, 10]):
W' = [0.5, 0.3, 4.0]  # 양자화 후 [0, 0, 4]
X' = [0.1, 0.1, 1.0]
→ 출력 오류 = 0.4 (10배 감소!)
```

### 3.3 등가 변환

```
원본:        Q(W) @ X
스케일링:    Q(W·diag(s)) @ diag(s)^{-1} @ X

중요: 스케일링은 수학적으로 동등하지만,
      양자화 후 오류가 다름!
```

---

## 4. 최적 스케일 찾기

### 4.1 목표

양자화 오류 최소화:

$$s^* = \arg\min_s \|Q(W \cdot s)(s^{-1} \cdot X) - WX\|$$

### 4.2 검색 기반 방법

모든 채널에 대해 최적 스케일 검색:

```python
def find_optimal_scale(W, X, n_grid=20):
    """
    각 채널의 최적 스케일을 그리드 서치로 찾기
    """
    best_scales = []

    for channel in range(W.shape[1]):
        best_error = float('inf')
        best_s = 1.0

        # 스케일 후보들
        for s in torch.linspace(0.1, 2.0, n_grid):
            # 스케일 적용
            W_scaled = W.clone()
            W_scaled[:, channel] *= s

            # 양자화
            W_quant = quantize(W_scaled)

            # 오류 계산
            X_scaled = X.clone()
            X_scaled[channel] /= s
            error = (W_quant @ X_scaled - W @ X).pow(2).mean()

            if error < best_error:
                best_error = error
                best_s = s

        best_scales.append(best_s)

    return torch.tensor(best_scales)
```

### 4.3 Activation-aware Scaling

활성화 크기에 비례하는 스케일 설정:

$$s_j = \left(\frac{\bar{X}_j}{\max(\bar{X})}\right)^\alpha$$

여기서:
- $\bar{X}_j$: j번째 채널의 평균 활성화 크기
- $\alpha$: 스케일링 강도 (기본 0.5)

```python
# 간단한 구현
activation_scale = X.abs().mean(dim=0)  # 채널별 평균
s = (activation_scale / activation_scale.max()) ** alpha
```

### 4.4 α 값의 영향

```
α = 0:   스케일링 없음 (s = 1)
α = 0.5: 적당한 스케일링 (권장)
α = 1:   강한 스케일링

너무 크면: 작은 활성화 채널의 가중치가 너무 작아짐
너무 작으면: salient weight 보호 효과 없음
```

---

## 5. AWQ 전체 알고리즘

### 5.1 전체 과정

```
알고리즘: AWQ
─────────────────────
입력: 모델 M, calibration 데이터 D
출력: 양자화된 모델 M_q

1. for each layer in M:
2.     # 활성화 수집
3.     X = collect_activations(layer, D)
4.
5.     for each linear module in layer:
6.         # 채널별 활성화 크기 계산
7.         act_scale = X.abs().mean(dim=0)
8.
9.         # 스케일 계산
10.        s = (act_scale / act_scale.max()) ** α
11.
12.        # 가중치 스케일링
13.        W_scaled = W * s
14.
15.        # 양자화
16.        W_quant = quantize(W_scaled)
17.
18.        # 역스케일을 다음 레이어 또는 활성화에 적용
19.        apply_inverse_scale(s)
20.
21.    end for
22. end for

return M_q
```

### 5.2 스케일 흡수

스케일을 모델에 영구적으로 흡수:

```python
# LayerNorm → Linear 구조에서
# s를 LayerNorm의 weight에 흡수

layernorm.weight = layernorm.weight / s

# 또는 이전 Linear의 bias에 흡수
# 이렇게 하면 추론 시 추가 연산 없음!
```

---

## 6. 쉬운 예시로 이해하기

### 6.1 학생 점수 비유

학생 10명의 점수를 1-10 스케일로 저장 (정밀도 한계):

**기존 방식**:
- 점수: [95, 92, 5, 88, 91, ...]
- 모두 같은 스케일로 양자화
- 5점 학생 정보 손실

**AWQ 방식**:
- "5점 학생이 중요한 정보를 가지고 있음" 발견
- 5점 × 10 = 50으로 스케일업
- [95, 92, 50, 88, 91, ...] 양자화
- 나중에 50 / 10 = 5로 복원
- 정보 보존!

### 6.2 사진 편집 비유

어두운 부분의 디테일이 중요한 사진:

**기존 압축**:
- 전체 같은 방식으로 압축
- 어두운 부분 디테일 손실

**AWQ 방식**:
- 중요한 어두운 영역 밝기 증가 (스케일업)
- 압축 수행 (디테일 보존)
- 다시 어두움 복원 (스케일다운)

---

## 7. 구현 세부사항

### 7.1 Group Quantization

```python
# AWQ도 group-wise quantization 지원
group_size = 128

for i in range(0, n_channels, group_size):
    group_weights = W[:, i:i+group_size]
    group_scale = compute_awq_scale(group_weights, X[i:i+group_size])
    # ...
```

### 7.2 효율적인 커널

AWQ의 양자화된 가중치를 위한 CUDA/Triton 커널:

```python
# AutoAWQ 사용
from awq import AutoAWQForCausalLM

# 양자화된 모델 로드
model = AutoAWQForCausalLM.from_quantized(
    "TheBloke/Llama-2-7B-AWQ",
    fuse_layers=True  # 레이어 퓨전으로 속도 향상
)
```

### 7.3 양자화 시간 비교

| 모델 | GPTQ | AWQ |
|------|------|-----|
| LLaMA-7B | 5분 | **3분** |
| LLaMA-13B | 10분 | **5분** |
| LLaMA-70B | 1시간 | **20분** |

AWQ가 ~3배 빠름 (Hessian 계산 불필요)

---

## 8. 실험 결과

### 8.1 Perplexity 비교

| 모델 | FP16 | RTN | GPTQ | **AWQ** |
|------|------|-----|------|---------|
| LLaMA-7B | 5.68 | 6.24 | 5.85 | **5.78** |
| LLaMA-13B | 5.09 | 5.53 | 5.20 | **5.19** |
| LLaMA-30B | 4.10 | 4.54 | 4.22 | **4.21** |
| LLaMA-65B | 3.53 | 3.92 | 3.65 | **3.64** |

**AWQ ≈ GPTQ ≤ FP16**

### 8.2 Zero-shot 정확도

| 모델 | Bits | WinoGrande | HellaSwag | PIQA |
|------|------|------------|-----------|------|
| LLaMA-7B | FP16 | 70.0 | 76.2 | 79.2 |
| LLaMA-7B | AWQ 4bit | **69.5** | **75.5** | **78.7** |

### 8.3 추론 속도

A100 GPU에서 LLaMA-13B:

| 배치 크기 | FP16 | AWQ 4bit | 속도 향상 |
|-----------|------|----------|-----------|
| 1 | 기준 | 1.45× | **1.45×** |
| 4 | 기준 | 1.83× | **1.83×** |
| 16 | 기준 | 2.11× | **2.11×** |

### 8.4 메모리 효율

| 모델 | FP16 | AWQ 4bit | 절약 |
|------|------|----------|------|
| LLaMA-7B | 14GB | 4GB | 3.5× |
| LLaMA-70B | 140GB | 36GB | **3.9×** |

---

## 9. 실무 사용법

### 9.1 AutoAWQ로 양자화

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 모델 로드
model_path = "meta-llama/Llama-2-7b-hf"
model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 양자화 설정
quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
    "version": "GEMM"
}

# 양자화 수행
model.quantize(tokenizer, quant_config=quant_config)

# 저장
model.save_quantized("llama-2-7b-awq")
tokenizer.save_pretrained("llama-2-7b-awq")
```

### 9.2 양자화된 모델 사용

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 로드
model = AutoAWQForCausalLM.from_quantized(
    "TheBloke/Llama-2-7B-AWQ",
    fuse_layers=True
)
tokenizer = AutoTokenizer.from_pretrained(
    "TheBloke/Llama-2-7B-AWQ"
)

# 추론
prompt = "The meaning of life is"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### 9.3 vLLM 통합

```python
from vllm import LLM

llm = LLM(
    model="TheBloke/Llama-2-13B-AWQ",
    quantization="awq",
    dtype="float16"
)

outputs = llm.generate(["Hello world"])
```

---

## 10. AWQ vs GPTQ 상세 비교

### 10.1 방법론 비교

| 측면 | GPTQ | AWQ |
|------|------|-----|
| 핵심 아이디어 | 오류 보상 | 중요 채널 스케일링 |
| Hessian 필요 | 예 | 아니오 |
| 양자화 순서 | 순차적 | 병렬 가능 |
| 직관성 | 낮음 | 높음 |

### 10.2 결과 비교

| 측면 | GPTQ | AWQ |
|------|------|-----|
| 정확도 | 좋음 | **약간 더 좋음** |
| 속도 | 빠름 | **더 빠름** |
| 양자화 시간 | 길음 | **짧음** |
| 구현 복잡도 | 높음 | 낮음 |

### 10.3 선택 가이드

**AWQ 선택**:
- 빠른 양자화 필요
- 간단한 구현 원함
- 최신 모델 지원 필요

**GPTQ 선택**:
- 더 많은 레거시 지원
- 특정 하드웨어 최적화 커널

---

## 11. 한계점 및 후속 연구

### 11.1 한계점

1. **Calibration 데이터 의존성**:
   - 도메인에 따라 최적 스케일 다름

2. **3bit 이하**:
   - 정확도 손실 여전히 있음

3. **활성화 양자화 미포함**:
   - 가중치만 양자화

### 11.2 후속 연구

- **QuIP** (2023): Incoherence processing
- **AQLM** (2024): 2bit까지 효과적
- **SmoothQuant** (조합): AWQ + 활성화 양자화

---

## 12. 핵심 요약

### 기억해야 할 것들

1. **핵심 통찰**: 큰 활성화와 곱해지는 가중치가 중요
2. **해결책**: Per-channel scaling으로 중요 가중치 보호
3. **장점**: GPTQ보다 빠르고 정확
4. **실용성**: 간단한 구현, 빠른 양자화

### AWQ의 핵심 공식

스케일 계산:
$$s_j = \left(\frac{\bar{X}_j}{\max(\bar{X})}\right)^\alpha$$

등가 변환:
$$Q(W \cdot s) \cdot (s^{-1} \cdot X) \approx W \cdot X$$

### 빠른 비교

```
양자화 시간:  AWQ < GPTQ
정확도:      AWQ ≥ GPTQ
구현 복잡도:  AWQ < GPTQ
하드웨어 지원: AWQ ≈ GPTQ
```

---

## 참고 자료

1. [AWQ 논문](https://arxiv.org/abs/2306.00978)
2. [공식 GitHub](https://github.com/mit-han-lab/llm-awq)
3. [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)
4. [Song Han 교수 강의](https://www.youtube.com/watch?v=WpqVSO-5vRo)

---

*이전 리뷰: [GPTQ](./002_GPTQ.md)*
*다음 리뷰: [SmoothQuant](./004_SmoothQuant.md)*
