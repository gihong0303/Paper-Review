# IPO: A General Theoretical Paradigm to Understand Learning from Human Preferences

**논문 발표**: 2023년
**저자**: Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos
**소속**: DeepMind
**논문 링크**: [arXiv:2310.12036](https://arxiv.org/abs/2310.12036)

---

## 한 줄 요약
> DPO의 과적합 문제를 이론적으로 분석하고, Identity mapping을 사용한 새로운 loss로 더 안정적인 선호도 학습 달성

---

## 1. DPO의 문제점

### 1.1 과적합 (Overfitting)

DPO는 deterministic preference에 과적합:
$$\pi_\theta(y_w|x) \to 1, \quad \pi_\theta(y_l|x) \to 0$$

### 1.2 Length Exploitation

긴 응답이 유리 → 불필요하게 긴 응답 생성

---

## 2. IPO Loss

### 2.1 수식

$$\mathcal{L}_{\text{IPO}} = \left(\log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{2\tau}\right)^2$$

### 2.2 vs DPO

| DPO | IPO |
|-----|-----|
| Log-sigmoid | **Squared** |
| 극단적 확률 | **완만한 변화** |
| 과적합 위험 | **안정적** |

---

## 3. 실험 결과

| 방법 | TL;DR | Anthropic-HH |
|------|-------|--------------|
| DPO | 기준 | 기준 |
| **IPO** | **+2%** | **+3%** |

---

## 4. 핵심 요약

1. **문제**: DPO 과적합
2. **해결**: Squared loss로 완만한 최적화
3. **결과**: 더 안정적인 학습

---

## 참고 자료

1. [IPO 논문](https://arxiv.org/abs/2310.12036)

---

*이전 리뷰: [DPO](./002_DPO.md)*
*다음 리뷰: [KTO](./004_KTO.md)*
