# Neural Machine Translation of Rare Words with Subword Units (BPE)

**논문 발표**: 2016년 (ACL 2016)
**저자**: Rico Sennrich, Barry Haddow, Alexandra Birch
**소속**: University of Edinburgh
**논문 링크**: [arXiv:1508.07909](https://arxiv.org/abs/1508.07909)

---

## 한 줄 요약
> 데이터 압축 알고리즘 BPE를 텍스트 토큰화에 적용하여, OOV(Out-of-Vocabulary) 문제를 해결하고 희귀 단어를 subword 단위로 분해하는 기법

---

## 1. 문제: OOV와 희귀 단어

### 1.1 단어 단위 토큰화의 한계

```
어휘집: ["cat", "dog", "running", ...]

문제 1 - OOV:
입력: "cryptocurrency"
결과: <UNK> (처리 불가!)

문제 2 - 어휘 폭발:
모든 단어 포함 시 → 수백만 개
메모리/속도 문제
```

### 1.2 문자 단위의 한계

```
문자 단위 토큰화:
"running" → ["r", "u", "n", "n", "i", "n", "g"]

문제:
- 시퀀스가 너무 길어짐
- 의미 정보 손실
```

### 1.3 Subword의 해결책

```
Subword 토큰화:
"running" → ["run", "ning"]
"cryptocurrency" → ["crypto", "currency"]

장점:
- OOV 없음
- 적절한 시퀀스 길이
- 의미 보존
```

---

## 2. BPE 알고리즘

### 2.1 핵심 아이디어

```
데이터 압축에서 차용:
가장 빈번한 문자 쌍을 반복적으로 병합
```

### 2.2 알고리즘 단계

```python
def learn_bpe(corpus, num_merges):
    # 1. 초기화: 문자 단위로 분할
    vocab = {word: list(word) + ['</w>'] for word in corpus}

    for i in range(num_merges):
        # 2. 가장 빈번한 쌍 찾기
        pairs = count_pairs(vocab)
        best_pair = max(pairs, key=pairs.get)

        # 3. 병합
        vocab = merge_pair(vocab, best_pair)

        # 4. 규칙 저장
        save_rule(best_pair)

    return vocab
```

### 2.3 예시

```
코퍼스: ["low", "lowest", "newer", "wider"]

초기:
low:    l o w </w>
lowest: l o w e s t </w>
newer:  n e w e r </w>
wider:  w i d e r </w>

병합 1: (e, r) → er (빈도 2)
병합 2: (e, w) → ew (빈도 2)
병합 3: (l, o) → lo (빈도 2)
병합 4: (lo, w) → low (빈도 2)
...

최종:
low:    low </w>
lowest: low est </w>
newer:  n ew er </w>
wider:  wid er </w>
```

---

## 3. 코드 구현

### 3.1 BPE 학습

```python
import re
from collections import Counter

def get_vocab(corpus):
    """단어 빈도 계산"""
    vocab = Counter()
    for sentence in corpus:
        for word in sentence.split():
            # 단어 끝에 </w> 추가
            word = ' '.join(list(word)) + ' </w>'
            vocab[word] += 1
    return vocab

def get_pairs(vocab):
    """모든 인접 쌍의 빈도 계산"""
    pairs = Counter()
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

def merge_vocab(pair, vocab):
    """선택된 쌍을 병합"""
    new_vocab = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)

    for word, freq in vocab.items():
        new_word = word.replace(bigram, replacement)
        new_vocab[new_word] = freq

    return new_vocab

def learn_bpe(corpus, num_merges):
    """BPE 학습"""
    vocab = get_vocab(corpus)
    merges = []

    for _ in range(num_merges):
        pairs = get_pairs(vocab)
        if not pairs:
            break

        best_pair = max(pairs, key=pairs.get)
        vocab = merge_vocab(best_pair, vocab)
        merges.append(best_pair)

    return vocab, merges
```

### 3.2 BPE 적용

```python
def apply_bpe(word, merges):
    """학습된 BPE를 새 단어에 적용"""
    word = list(word) + ['</w>']

    for pair in merges:
        i = 0
        while i < len(word) - 1:
            if word[i:i+2] == list(pair):
                word = word[:i] + [''.join(pair)] + word[i+2:]
            else:
                i += 1

    return word

# 예시
word = "newest"
tokens = apply_bpe(word, merges)
# ['new', 'est', '</w>']
```

---

## 4. BPE의 변형

### 4.1 WordPiece (BERT)

```
차이점:
- BPE: 빈도 기반 병합
- WordPiece: likelihood 기반 병합

$$\text{Score}(a, b) = \frac{\text{freq}(ab)}{\text{freq}(a) \times \text{freq}(b)}$$
```

### 4.2 Unigram (SentencePiece)

```
차이점:
- BPE: bottom-up (문자 → subword)
- Unigram: top-down (전체 → 최적 분할)
```

### 4.3 Byte-level BPE (GPT-2/3/4)

```python
# 바이트 단위로 시작
# UTF-8 바이트 (0-255)를 기본 vocabulary로 사용

# 장점:
# - 어떤 문자도 처리 가능
# - 다국어 지원
# - OOV 완전 제거
```

---

## 5. 실험 결과

### 5.1 기계 번역 (WMT)

| 방법 | BLEU (En→De) |
|------|--------------|
| Word-based | 21.7 |
| Character | 20.5 |
| **BPE** | **24.5** |

### 5.2 희귀 단어 번역

```
예시: "skateboard" (학습 데이터에 없음)

Word-based: <UNK> → 번역 실패
BPE: "skate" + "board" → 올바른 번역!
```

---

## 6. BPE in LLMs

### 6.1 사용 현황

| 모델 | 토크나이저 | Vocab 크기 |
|------|------------|------------|
| GPT-2/3/4 | Byte-level BPE | 50K |
| LLaMA | BPE (SentencePiece) | 32K |
| RoBERTa | Byte-level BPE | 50K |

### 6.2 Vocabulary 크기 선택

```
작은 vocab (8K):
- 시퀀스 길어짐
- 메모리 효율적

큰 vocab (100K):
- 시퀀스 짧아짐
- 임베딩 메모리 증가
- 희귀 토큰 학습 어려움

적정 범위: 32K ~ 64K
```

---

## 7. 한국어와 BPE

### 7.1 문제점

```
영어 중심 BPE의 한국어 처리:
"안녕하세요" → ["안", "녕", "하", "세", "요"]

문제:
- 거의 문자 단위로 분해
- 토큰 효율 낮음
- 의미 정보 손실
```

### 7.2 해결책

```python
# 한국어 특화 vocabulary 추가
korean_vocab = [
    "안녕",
    "하세요",
    "감사",
    "합니다",
    ...
]

# 또는 한국어 코퍼스로 BPE 재학습
korean_corpus = load_korean_data()
vocab, merges = learn_bpe(korean_corpus, 32000)
```

---

## 8. 쉬운 예시

### 8.1 레고 블록 비유

```
단어 = 레고 작품
Subword = 기본 블록

새로운 작품이 필요할 때:
- 단어 방식: 완제품 구매 필요 (없으면 불가)
- BPE 방식: 기존 블록으로 조립 가능

"cryptocurrency" = "crypto" + "currency"
새 단어도 기존 블록으로 조립!
```

### 8.2 한글 자모 비유

```
한글의 원리와 유사:
자음 + 모음 = 글자

ㄱ + ㅏ + ㄴ = 간
ㅁ + ㅜ + ㄹ = 물

BPE:
sub + word = subword
un + known = unknown
```

---

## 9. 장단점

### 9.1 장점

```
1. OOV 문제 해결
2. 어휘 크기 제어 가능
3. 의미 있는 subword 학습
4. 언어 독립적
5. 구현 간단
```

### 9.2 단점

```
1. 언어별 최적화 필요
2. 분할 모호성
   "서울시" → "서울" + "시" or "서" + "울시"?
3. 빈도 편향
   빈번한 단어는 통째로, 희귀 단어는 잘게
```

---

## 10. 핵심 요약

### 기억해야 할 것들

1. **핵심**: 빈번한 쌍을 반복 병합
2. **장점**: OOV 해결, 어휘 크기 제어
3. **사용**: GPT, LLaMA 등 대부분의 LLM
4. **변형**: WordPiece, Unigram, Byte-level BPE

### 알고리즘 요약

```
1. 문자 단위로 초기화
2. 가장 빈번한 쌍 찾기
3. 쌍을 병합
4. 2-3을 N번 반복
5. 결과: 최종 vocabulary
```

### 수식

$$\text{Best Pair} = \arg\max_{(a,b)} \text{count}(a, b)$$

### 실무 팁

- Vocab 크기: 32K~64K 권장
- 한국어: 별도 학습 또는 확장
- Byte-level: 다국어에 적합
- 도메인 특화: 해당 코퍼스로 학습

---

## 참고 자료

1. [BPE 논문](https://arxiv.org/abs/1508.07909)
2. [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers)
3. [OpenAI tiktoken](https://github.com/openai/tiktoken)

---

*다음 리뷰: [SentencePiece](./002_SentencePiece.md)*
