# GQA: Grouped-Query Attention

**논문 발표**: 2023년 (EMNLP 2023)
**저자**: Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yinfei Yang, Sunipa Dev, Santiago Ontañón
**소속**: Google Research
**논문 링크**: [arXiv:2305.13245](https://arxiv.org/abs/2305.13245)

---

## 한 줄 요약
> Multi-Head Attention과 Multi-Query Attention의 중간 형태로, KV 캐시를 줄이면서 MHA에 가까운 품질 유지

---

## 1. 세 가지 Attention

### 1.1 비교

```
Multi-Head Attention (MHA):
Q: H heads, K: H heads, V: H heads

Multi-Query Attention (MQA):
Q: H heads, K: 1 head, V: 1 head

Grouped-Query Attention (GQA):
Q: H heads, K: G groups, V: G groups
```

### 1.2 시각화

```
MHA (8 heads):
Q: ████████ (8)
K: ████████ (8)
V: ████████ (8)

MQA:
Q: ████████ (8)
K: █ (1)
V: █ (1)

GQA (2 groups):
Q: ████████ (8)
K: ██ (2)
V: ██ (2)
```

---

## 2. 구현

```python
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, num_heads, num_kv_heads):
        super().__init__()
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = d_model // num_heads

        # Q는 모든 head
        self.W_q = nn.Linear(d_model, num_heads * self.head_dim)
        # K, V는 적은 head
        self.W_k = nn.Linear(d_model, num_kv_heads * self.head_dim)
        self.W_v = nn.Linear(d_model, num_kv_heads * self.head_dim)

    def forward(self, x):
        B, L, _ = x.shape

        Q = self.W_q(x).view(B, L, self.num_heads, self.head_dim)
        K = self.W_k(x).view(B, L, self.num_kv_heads, self.head_dim)
        V = self.W_v(x).view(B, L, self.num_kv_heads, self.head_dim)

        # K, V를 Q heads에 맞게 반복
        K = K.repeat_interleave(self.num_heads // self.num_kv_heads, dim=2)
        V = V.repeat_interleave(self.num_heads // self.num_kv_heads, dim=2)

        # Attention
        attn = scaled_dot_product_attention(Q, K, V)
        return attn
```

---

## 3. Uptraining

### 3.1 기존 MHA → GQA 변환

```
1. MHA 모델에서 시작
2. K, V head를 그룹으로 병합 (평균)
3. 적은 토큰으로 추가 학습
```

### 3.2 변환 예시

```
MHA 8 heads → GQA 2 groups:

기존 K heads: K1, K2, K3, K4, K5, K6, K7, K8
그룹화: G1 = mean(K1,K2,K3,K4), G2 = mean(K5,K6,K7,K8)
```

---

## 4. 효과

### 4.1 KV 캐시 절약

```
MHA: 8 heads → 8 KV
GQA: 8 Q, 2 KV → 4배 절약

LLaMA 2 70B:
- 64 Q heads, 8 KV heads
- 8배 KV 캐시 절약!
```

### 4.2 성능 비교

| 방법 | 품질 | 추론 속도 |
|------|------|-----------|
| MHA | 최고 | 느림 |
| MQA | 낮음 | 빠름 |
| **GQA** | **MHA 근접** | **빠름** |

---

## 5. 사용 모델

| 모델 | Q Heads | KV Heads |
|------|---------|----------|
| LLaMA 2 70B | 64 | 8 |
| LLaMA 3 8B | 32 | 8 |
| LLaMA 3 70B | 64 | 8 |
| Mistral 7B | 32 | 8 |

---

## 6. 핵심 요약

### 기억해야 할 것들

1. **핵심**: MHA와 MQA의 중간 형태
2. **효과**: KV 캐시 절약 + 품질 유지
3. **방법**: K, V heads를 그룹화
4. **사용**: LLaMA 2/3, Mistral

### 실무 팁

```
일반적 설정:
- Q heads: 32-128
- KV heads: 8

→ 4-16배 KV 캐시 절약
```

---

## 참고 자료

1. [GQA 논문](https://arxiv.org/abs/2305.13245)

---

*이전 리뷰: [ALiBi](./002_ALiBi.md)*
*다음 리뷰: [Sliding Window Attention](./004_Sliding_Window.md)*
