# Textbooks Are All You Need II: Phi-1.5

**논문 발표**: 2023년
**저자**: Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee
**소속**: Microsoft Research
**논문 링크**: [arXiv:2309.05463](https://arxiv.org/abs/2309.05463)
**공식 모델**: [HuggingFace](https://huggingface.co/microsoft/phi-1_5)

---

## 한 줄 요약
> Phi-1의 "교과서 방식"을 코드에서 상식 추론으로 확장하여, 1.3B 파라미터로 LLaMA-2 7B와 비슷한 추론 성능 달성

---

## 1. Phi-1에서 Phi-1.5로

### 1.1 Phi-1의 한계

```
Phi-1: 코드만 잘함
- HumanEval: 50.6%
- 일반 상식: 약함
- 언어 이해: 제한적
```

### 1.2 Phi-1.5의 목표

```
코드 능력 유지 + 상식 추론 추가

"교과서 방식"을 자연어로 확장
```

---

## 2. 데이터 구성

### 2.1 학습 데이터

| 데이터셋 | 토큰 수 | 내용 |
|----------|---------|------|
| Phi-1 데이터 | 7B | 코드 교과서 |
| 새 합성 데이터 | 20B | NLP 교과서 |
| 필터링 웹 | 3B | 교육적 웹 콘텐츠 |
| **총계** | **30B** | - |

### 2.2 새로운 합성 데이터

```python
# 상식 추론 데이터 생성
topics = [
    "물리학 기초",
    "일상 추론",
    "인과관계 이해",
    "수학적 사고",
    ...
]

for topic in topics:
    # 교과서 스타일로 생성
    content = generate_textbook_content(topic)
    # 연습문제 추가
    exercises = generate_exercises(topic)
    dataset.add(content + exercises)
```

### 2.3 합성 데이터 예시

```markdown
## 그림자와 빛

빛이 물체에 의해 차단되면 그림자가 생깁니다.

### 핵심 개념
1. 빛은 직선으로 이동합니다
2. 불투명 물체는 빛을 차단합니다
3. 그림자 크기는 광원 거리에 따라 달라집니다

### 예제
문제: 손전등을 공에 비추면 벽에 그림자가 생깁니다.
손전등을 공에 가까이 가져가면 그림자는 어떻게 되나요?

답: 그림자가 커집니다. 광원이 물체에 가까워지면
차단되는 빛의 각도가 커져서 그림자가 확대됩니다.
```

---

## 3. 모델 아키텍처

### 3.1 Phi-1.5 스펙

| 항목 | Phi-1 | Phi-1.5 |
|------|-------|---------|
| 파라미터 | 1.3B | 1.3B |
| 레이어 | 24 | 24 |
| Hidden | 2048 | 2048 |
| Context | 2048 | 2048 |

**동일한 아키텍처, 다른 데이터!**

### 3.2 학습 설정

```python
training_config = {
    "total_tokens": 150B,  # 30B × 5 에폭
    "batch_size": 2048,
    "learning_rate": 2e-4,
    "warmup_steps": 1000,
    "fp16": True
}
```

---

## 4. 실험 결과

### 4.1 상식 추론 (Common Sense)

| 모델 | 파라미터 | WinoGrande | ARC-Easy | ARC-Challenge | PIQA |
|------|----------|------------|----------|---------------|------|
| LLaMA-2 | 7B | 69.2 | 76.3 | 45.9 | 79.1 |
| Falcon | 7B | 66.3 | 74.6 | 42.4 | 79.4 |
| **Phi-1.5** | **1.3B** | **72.2** | **76.7** | **44.8** | **76.6** |

**5배 작은 모델이 비슷한 성능!**

### 4.2 코드 성능 유지

| 모델 | HumanEval | MBPP |
|------|-----------|------|
| Phi-1 | 50.6% | 44.8% |
| Phi-1.5 | 41.4% | 43.5% |

약간 하락했지만 여전히 우수

### 4.3 수학 추론 (GSM8K)

| 모델 | 파라미터 | GSM8K |
|------|----------|-------|
| LLaMA-2 | 7B | 14.6% |
| Falcon | 7B | 6.8% |
| **Phi-1.5** | **1.3B** | **40.2%** |

---

## 5. 왜 효과적인가?

### 5.1 교과서 데이터의 특성

```
1. 구조화된 지식
   - 개념 → 예제 → 연습
   - 단계별 설명
   - 명확한 인과관계

2. 다양한 추론 패턴
   - 연역적 추론
   - 귀납적 추론
   - 유추

3. 오류 방지
   - 흔한 실수 설명
   - 반례 제시
```

### 5.2 합성 데이터의 장점

```
웹 데이터 문제:
- 불완전한 설명
- 잘못된 정보
- 맥락 없는 사실

합성 데이터 장점:
- 완전한 설명
- 검증된 정보
- 풍부한 맥락
```

---

## 6. 한계와 주의사항

### 6.1 현재 한계

1. **사실 정확성**: 환각(hallucination) 여전히 존재
2. **긴 텍스트**: 2048 토큰 제한
3. **최신 지식**: 학습 데이터 이후 정보 없음

### 6.2 독성 콘텐츠

```
Phi-1.5는 해로운 콘텐츠 생성을 쉽게 유도할 수 있음
→ Safety fine-tuning이 포함되지 않았기 때문

사용 시 주의 필요!
```

---

## 7. 사용법

### 7.1 기본 사용

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-1_5",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")

# 추론
prompt = "Explain why the sky is blue in simple terms."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
```

### 7.2 Q&A 형식

```python
prompt = """Question: If I drop a ball and a feather from the same height
in a vacuum, which hits the ground first?

Answer: In a vacuum,"""

# 모델이 완성: "both hit the ground at the same time because..."
```

---

## 8. Phi 시리즈 발전

### 8.1 시리즈 개요

```
Phi-1 (2023.06)
  ↓ 코드 특화
Phi-1.5 (2023.09)
  ↓ + 상식 추론
Phi-2 (2023.12)
  ↓ 2.7B, 더 강력
Phi-3 (2024.04)
  ↓ 3.8B, SOTA 소형 모델
```

### 8.2 핵심 교훈

```
1. 데이터 품질 > 데이터 양
2. 합성 데이터도 매우 효과적
3. 작은 모델도 충분히 강력
4. 교과서 스타일이 학습에 최적
```

---

## 9. 쉬운 예시

### 9.1 학습 방식 비유

```
일반 LLM 학습:
- 인터넷 전체 글 읽기
- 좋은 것, 나쁜 것 섞여 있음
- 양은 많지만 질은 들쭉날쭉

Phi-1.5 학습:
- 최고의 교과서만 선별
- 체계적이고 명확한 설명
- 양은 적지만 질이 높음
```

### 9.2 대학생 vs 독학생 비유

```
일반 LLM:
- 도서관의 모든 책 무작위 읽기
- 혼란스럽고 비효율적

Phi-1.5:
- 전공 교수님이 추천한 교재만
- 체계적이고 효율적
```

---

## 10. 핵심 요약

### 기억해야 할 것들

1. **확장**: 코드 → 상식 추론
2. **데이터**: 30B 합성 토큰
3. **결과**: 1.3B로 7B급 성능
4. **핵심**: 교과서 품질 데이터

### 주요 수치 비교

| 모델 | 파라미터 | 학습 토큰 | 추론 성능 |
|------|----------|-----------|-----------|
| LLaMA-2 | 7B | 2T | 기준 |
| Phi-1.5 | 1.3B | 30B | ~동등 |

효율성: **~350배**

### 실무 팁

- 합성 데이터 생성 시 "교과서 스타일" 유지
- 개념 → 예제 → 연습 구조
- 다양한 추론 유형 포함
- Safety fine-tuning 필요

---

## 참고 자료

1. [Phi-1.5 논문](https://arxiv.org/abs/2309.05463)
2. [HuggingFace](https://huggingface.co/microsoft/phi-1_5)
3. [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/)

---

*이전 리뷰: [Phi-1](./001_Phi-1_Textbooks.md)*
*다음 리뷰: [FineWeb](./003_FineWeb.md)*
