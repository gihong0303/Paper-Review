# Language Models are Few-Shot Learners (GPT-3)

**논문 발표**: 2020년 (NeurIPS 2020)
**저자**: Tom B. Brown, Benjamin Mann, Nick Ryder, et al.
**소속**: OpenAI
**논문 링크**: [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)

---

## 한 줄 요약
> 175B 파라미터의 거대 언어 모델이 fine-tuning 없이 few-shot 예시만으로 다양한 태스크를 수행할 수 있음을 입증

---

## 1. 핵심 발견: In-Context Learning

### 1.1 새로운 패러다임

```
기존 방식:
Pre-train → Fine-tune → 태스크별 모델

GPT-3 방식:
Pre-train → 프롬프트만으로 모든 태스크!
```

### 1.2 세 가지 설정

```python
# Zero-shot
prompt = "Translate English to French: cheese =>"

# One-shot
prompt = """Translate English to French:
sea otter => loutre de mer
cheese =>"""

# Few-shot
prompt = """Translate English to French:
sea otter => loutre de mer
peppermint => menthe poivrée
plush giraffe => girafe en peluche
cheese =>"""
```

---

## 2. 모델 규모

### 2.1 GPT-3 변형

| 모델 | 파라미터 | 레이어 | d_model | Heads |
|------|----------|--------|---------|-------|
| Small | 125M | 12 | 768 | 12 |
| Medium | 350M | 24 | 1024 | 16 |
| Large | 760M | 24 | 1536 | 16 |
| XL | 1.3B | 24 | 2048 | 24 |
| 2.7B | 2.7B | 32 | 2560 | 32 |
| 6.7B | 6.7B | 32 | 4096 | 32 |
| 13B | 13B | 40 | 5140 | 40 |
| **175B** | **175B** | **96** | **12288** | **96** |

### 2.2 학습 데이터

| 데이터셋 | 토큰 | 비율 |
|----------|------|------|
| Common Crawl (필터링) | 410B | 60% |
| WebText2 | 19B | 22% |
| Books1 | 12B | 8% |
| Books2 | 55B | 8% |
| Wikipedia | 3B | 3% |

---

## 3. 핵심 결과

### 3.1 스케일에 따른 Few-shot

```
모델 크기 증가 → Few-shot 성능 급증

예: 산술 (2자리 덧셈)
- 13B: 59%
- 175B: 100%

In-context learning은 규모에서 나온다!
```

### 3.2 태스크별 성능

| 태스크 | Few-shot | Fine-tuned SOTA |
|--------|----------|-----------------|
| LAMBADA | 76.2% | 68.0% |
| TriviaQA | 71.2% | 68.0% |
| Translation (En→Fr) | 32.6 BLEU | 45.0 BLEU |

### 3.3 창발적 능력

```
큰 모델에서만 나타나는 능력:
- 3자리 산술
- 단어 unscrambling
- 코드 생성
```

---

## 4. In-Context Learning 이해

### 4.1 왜 작동하는가?

```
가설 1: 암묵적 fine-tuning
- Attention이 gradient descent와 유사

가설 2: 태스크 인식
- 학습 데이터에서 본 패턴 활성화

가설 3: 프롬프트 = 프로그램
- 예시가 "입출력 명세" 역할
```

### 4.2 예시 수의 영향

```
예시 수 vs 성능:
0 (zero-shot): 낮음
1 (one-shot): 급격히 상승
10-100 (few-shot): 점진적 상승

→ 첫 몇 개 예시가 가장 중요
```

---

## 5. 한계

### 5.1 구조적 한계

```
1. 양방향 컨텍스트 없음 (vs BERT)
2. 컨텍스트 길이 제한 (2048 토큰)
3. 연속 텍스트 처리 어려움
```

### 5.2 태스크별 한계

```
어려운 태스크:
- 자연어 추론 (ANLI)
- 상식 추론 일부
- 복잡한 비교

→ Fine-tuning이 여전히 필요한 경우
```

---

## 6. 쉬운 예시

### 6.1 시험 비유

```
Fine-tuning = 과목별 공부
- 수학 시험 → 수학 공부
- 영어 시험 → 영어 공부

Few-shot = 시험지에 예제 문제
- 예제 보고 풀이 방식 파악
- 바로 본 문제 풀기
```

### 6.2 요리 비유

```
Fine-tuning = 요리 학원 등록
- 파스타 과정, 한식 과정...

Few-shot = 레시피 보고 바로 요리
- "이렇게 하면 되는구나"
- 비슷한 요리에 적용
```

---

## 7. 핵심 요약

### 기억해야 할 것들

1. **핵심**: Fine-tuning 없이 few-shot으로 태스크 수행
2. **규모**: 175B 파라미터
3. **발견**: 스케일이 in-context learning 가능하게 함
4. **의의**: 범용 AI 가능성 제시

### 영향

- ChatGPT, GPT-4의 기반
- Prompt Engineering 분야 탄생
- LLM 연구 방향 전환

---

## 참고 자료

1. [GPT-3 논문](https://arxiv.org/abs/2005.14165)
2. [OpenAI Blog](https://openai.com/blog/gpt-3-apps/)

---

*이전 리뷰: [Attention Is All You Need](./001_Attention_Is_All_You_Need.md)*
*다음 리뷰: [Scaling Laws](./003_Scaling_Laws.md)*
