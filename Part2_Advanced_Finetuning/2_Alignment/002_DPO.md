# DPO: Direct Preference Optimization

**논문 발표**: 2023년 (NeurIPS 2023)
**저자**: Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn
**소속**: Stanford University
**논문 링크**: [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)

---

## 한 줄 요약
> Reward model과 강화학습(PPO) 없이, 선호도 데이터만으로 직접 정책을 최적화하여, RLHF와 동등한 성능을 더 간단하고 안정적으로 달성

---

## 1. RLHF의 문제점

```
RLHF 파이프라인:
1. SFT 모델 학습
2. Reward 모델 학습
3. PPO로 강화학습 ← 복잡하고 불안정!

문제:
- 3개의 모델 (SFT, RM, Policy)
- PPO 하이퍼파라미터 튜닝 어려움
- 학습 불안정
```

---

## 2. DPO의 핵심 아이디어

### 2.1 핵심 통찰

Reward model을 **정책으로 직접 표현**할 수 있음:

$$r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$

### 2.2 DPO Loss

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]$$

- $y_w$: 선호되는 응답 (winner)
- $y_l$: 비선호 응답 (loser)
- $\pi_{\text{ref}}$: Reference 모델 (SFT)

### 2.3 직관적 이해

```
DPO가 하는 것:
- 선호 응답(y_w)의 확률 ↑
- 비선호 응답(y_l)의 확률 ↓
- Reference에서 너무 벗어나지 않게 (β)
```

---

## 3. RLHF vs DPO

| 측면 | RLHF | DPO |
|------|------|-----|
| 모델 수 | 3개 | **1개** |
| 복잡성 | 높음 | **낮음** |
| 안정성 | 낮음 | **높음** |
| 하이퍼파라미터 | 많음 | **적음** |
| 성능 | 기준 | **동등** |

---

## 4. 구현

```python
import torch
import torch.nn.functional as F

def dpo_loss(policy_logps_w, policy_logps_l,
             ref_logps_w, ref_logps_l, beta=0.1):
    """
    policy_logps_w: π_θ(y_w|x)의 로그 확률
    policy_logps_l: π_θ(y_l|x)의 로그 확률
    ref_logps_w: π_ref(y_w|x)의 로그 확률
    ref_logps_l: π_ref(y_l|x)의 로그 확률
    """
    # Log ratio
    logits = beta * (
        (policy_logps_w - ref_logps_w) -
        (policy_logps_l - ref_logps_l)
    )

    # Binary cross entropy
    loss = -F.logsigmoid(logits).mean()
    return loss
```

---

## 5. 실험 결과

### 5.1 요약 태스크

| 방법 | Win Rate vs SFT |
|------|-----------------|
| PPO | 57% |
| **DPO** | **61%** |

### 5.2 대화 태스크

| 방법 | GPT-4 평가 |
|------|------------|
| RLHF | 기준 |
| **DPO** | **동등/우수** |

---

## 6. 핵심 요약

### 기억해야 할 것들

1. **핵심**: RL 없이 선호도 직접 최적화
2. **Loss**: Log-sigmoid of log-ratio difference
3. **장점**: 간단, 안정적, 효율적
4. **결과**: RLHF와 동등 성능

### β 선택

- β 낮음: 더 많이 변화 (risky)
- β 높음: reference에 가깝게 (conservative)
- 일반적: 0.1 - 0.5

---

## 참고 자료

1. [DPO 논문](https://arxiv.org/abs/2305.18290)
2. [HuggingFace TRL](https://github.com/huggingface/trl)

---

*이전 리뷰: [InstructGPT](./001_InstructGPT.md)*
*다음 리뷰: [IPO](./003_IPO.md)*
